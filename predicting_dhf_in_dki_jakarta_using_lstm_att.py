# -*- coding: utf-8 -*-
"""Predicting DHF in DKI Jakarta using LSTM-ATT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zu37FK2Gq0irueat91P5EJ_pFcxRyXU1

**ADHITYA DWI NUGRAHA**

**You can access the dataset through this link**

https://drive.google.com/file/d/1yCvI6B7AgdajjkW1F-VI_hHQsLtH2mQT/view?usp=drive_link

# **START**
"""

#Import library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
np.random.seed(42)

#Connecting to google drive
from google.colab import drive
drive.mount('/content/drive')

#Path to saved the dataset
path = "/content/drive/MyDrive/ICIAM data/merged_dataset.csv"

#Load dataset
data = pd.read_csv(path)

data.info()

"""# **OUTBREAK DHF**

Outbreak =  Current week value 2 times of previous week value
https://jdihn.go.id/files/667/SALINAN_PERWAL_30_PENGENDALIAN_PENYAKIT_DBD.pdf
"""

df = data.copy()

# Define the threshold for DHF outbreak
outbreak_threshold = 2

# Initialize a list to store outbreak status
outbreak_status = []

# Iterate through the DataFrame to detect outbreaks and set the status
for i in range(1, len(df)):
    current_week_cases = df.loc[i, 'DBD']
    prev_week_cases = df.loc[i - 1, 'DBD']

    if current_week_cases >= prev_week_cases * outbreak_threshold:
        outbreak_status.append('Yes')
    else:
        outbreak_status.append('No')

# Add the outbreak_status column to the DataFrame
df['outbreak_status'] = ['No'] + outbreak_status

# Print the DataFrame with the new column
print(df)

"""# **NEW TARGET DHF**

Target 1 = Average of previous week, current week, future week

Target 2 = Current week - previous week
"""

# Calculate Target_1 as the rolling average
df['Target_1'] = df['DBD'].rolling(window=3, min_periods=1).mean()

# Calculate Target_2 as the difference between consecutive weeks
df['Target_2'] = df['DBD'].diff()

# Replace NaN values in Target_2 with 0.1
df['Target_2'].fillna(1, inplace=True)

# Calculate Sum_Targets as the sum of Target_1 and Target_2
df['Sum_Targets'] = df['Target_1'] + df['Target_2']

"""# **NEW DATASET**"""

df.head()

df.describe()

"""# **PLOT DATA**

## **WEB VS IMAGE VS NEWS VS YOUTUBE**
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(45, 20))

# Plot Web
plt.plot(df['Week'], df['Web'], label='Web Score', marker='o', linestyle='-', linewidth=4)

# Plot Image
plt.plot(df['Week'], df['Image'], label='Image Score', marker='o', linestyle='--', linewidth=4)

# Plot News
plt.plot(df['Week'], df['News'], label='News Score', marker='o', linestyle='-.', linewidth=4)

# Plot Youtube
plt.plot(df['Week'], df['Youtube'], label='Youtube Score', marker='o', linestyle=':', linewidth=4)

# Add labels and title
plt.xlabel('Week')
plt.ylabel('Score')
plt.title('Score Comparison of 4 Internet Sources Through The Time')
plt.legend()

# Set legend font size
plt.legend(fontsize='large')

# Set x-axis range for better visibility
plt.xlim(1, 76)

# Show the plot
plt.show()

"""## DBD vs TARGET 1"""

import matplotlib.pyplot as plt


# Plotting the time series for DBD and target_1
plt.figure(figsize=(45, 20))

# Plot DBD cases
plt.plot(df['Week'], df['DBD'], label='DBD Cases', marker='o', linestyle=':', linewidth=4)

# Plot target_1
plt.plot(df['Week'], df['Target_1'], label='Target 1', marker='o', linestyle='-', linewidth=4)

# Add labels and title
plt.xlabel('Week')
plt.ylabel('Number of Cases')
plt.title('Time Series of DBD Cases and Target 1 in DKI Jakarta')
plt.legend()  # Show legend

# Set legend font size
plt.legend(fontsize='large')

# Show the plot
plt.show()

"""## **DBD vs TARGET 2**"""

import matplotlib.pyplot as plt


# Plotting the time series for DBD and target_2
plt.figure(figsize=(45, 20))

# Plot DBD cases
plt.plot(df['Week'], df['DBD'], label='DBD Cases', marker='o', linestyle=':', linewidth=4)

# Plot target_2
plt.plot(df['Week'], df['Target_2'], label='Target 2', marker='o', linestyle='-', linewidth=4)

# Add labels and title
plt.xlabel('Week')
plt.ylabel('Number of Cases')
plt.title('Time Series of DBD Cases and Target 2 in DKI Jakarta')
plt.legend()  # Show legend

# Set legend font size
plt.legend(fontsize='large')

# Show the plot
plt.show()

"""## **DBD vs SUM**"""

import matplotlib.pyplot as plt


# Plotting the time series for DBD and Sum_Target
plt.figure(figsize=(45, 20))

# Plot DBD cases
plt.plot(df['Week'], df['DBD'], label='DBD Cases', marker='o', linestyle=':', linewidth=4)

# Plot sum_target
plt.plot(df['Week'], df['Sum_Targets'], label='Sum_Targets', marker='o', linestyle='-', linewidth=4)

# Add labels and title
plt.xlabel('Week')
plt.ylabel('Number of Cases')
plt.title('Time Series of DBD Cases and Sum_Targets in DKI Jakarta')
plt.legend()  # Show legend

# Set legend font size
plt.legend(fontsize='large')

# Show the plot
plt.show()

"""# **MODEL EVALUATION**

## **ONLY DBD**
"""

import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import PoissonRegressor

data_pusat = df[['Week','Tavg','RH_avg','RR','DBD_lag','Score','DBD']]
data_pusat = data_pusat.set_index('Week')
data = data_pusat
n_features = data.shape[1]
n_outputs = 1

# Split data
train_data = data.iloc[:int(0.8*len(data)), :]    # Menentukan banyaknya data train yaitu sebesar 80% data
test_data = data.iloc[int(0.8*len(data)):, :]
train_data_raw, test_data_raw = train_data.copy(), test_data.copy()

#Scaling data
scaler = MinMaxScaler()
scaler = scaler.fit(train_data)
train_data = scaler.transform(train_data)
test_data = scaler.transform(test_data)

# Define a function to prepare the data
def prepare_data(data, n_steps):
    X, y = [], []
    for i in range(len(data)-n_steps):
        X.append(data[i:(i+n_steps), :])
        y.append(data[(i+n_steps), -1])
    return np.array(X), np.array(y)

# Define the number of time steps to look back
n_steps = 1

# Prepare the training data
X_train, y_train = prepare_data(train_data, n_steps)
X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).float()

# Prepare the testing data
X_test, y_test = prepare_data(test_data, n_steps)
X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).float()

# Define the LSTM model
class AttentionLSTM(nn.Module):
    def __init__(self, n_features, n_outputs, n_hidden=512, n_layers=3, attention_size=128):
        super().__init__()
        self.n_features = n_features
        self.n_hidden = n_hidden
        self.n_layers = n_layers
        self.n_outputs = n_outputs
        self.attention_size = attention_size

        self.lstm = nn.LSTM(input_size=n_features, hidden_size=n_hidden, num_layers=n_layers, batch_first=True)

        self.attention = nn.Linear(n_hidden, attention_size)
        self.projection = nn.Linear(attention_size, 1, bias=False)
        self.regressor = nn.Linear(n_hidden, n_outputs)

    def forward(self, x):
        output, hidden = self.lstm(x)

        attention_input = torch.tanh(self.attention(output))
        attention_scores = self.projection(attention_input)
        attention_weights = F.softmax(attention_scores, dim=1)

        context_vector = torch.bmm(output.transpose(1, 2), attention_weights).squeeze(2)
        output = self.regressor(context_vector)

        return output

# Initialize the model and define the optimizer and loss function
model = AttentionLSTM(n_features, n_outputs)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Train the model
train_loss = []
train_rmse = []
train_mae = []
n_epochs = 100
for epoch in range(n_epochs):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train.unsqueeze(1))
    loss.backward()
    optimizer.step()
    rmse = np.sqrt(mean_squared_error(y_train, outputs.detach().numpy()))
    mae = mean_absolute_error(y_train, outputs.detach().numpy())
    train_loss.append(loss.item())
    train_rmse.append(rmse)
    train_mae.append(mae)
    if epoch % 10 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}, RMSE: {:4f}'.format(epoch+1, n_epochs, loss.item(), rmse))

# Evaluate the model on the training data
with torch.no_grad():
    train_outputs = model(X_train)
    train_loss = criterion(train_outputs, y_train.unsqueeze(1))
    print('Train Loss: {:.4f}'.format(train_loss.item()))
    train_rmse = np.sqrt(mean_squared_error(y_train, train_outputs.numpy()))
    train_mae = mean_absolute_error(y_train, train_outputs.numpy())
    print('Train RMSE: {:.4f}'.format(train_rmse))
    print('Train MAE: {:.4f}'.format(train_mae))

# Evaluate the model on the testing data
with torch.no_grad():
    test_outputs = model(X_test)
    test_loss = criterion(test_outputs, y_test.unsqueeze(1))
    print('Test Loss: {:.4f}'.format(test_loss.item()))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_outputs.numpy()))
    test_mae = mean_absolute_error(y_test, test_outputs.numpy())
    print('Test RMSE: {:.4f}'.format(test_rmse))
    print('Test MAE: {:.4f}'.format(test_mae))

att_test_pred_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], test_outputs.detach().numpy().reshape(-1, 1))))[:,-1]
att_test_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], y_test.reshape(-1,1))))[:,-1]
test_rmse_denorm = np.sqrt(mean_squared_error(att_test_pred_rescaled, att_test_rescaled))
test_mae_denorm = mean_absolute_error(att_test_pred_rescaled, att_test_rescaled)
print('Test RMSE Denormalisasi: {:.4f}'.format(test_rmse_denorm))
print('Test MAE Denormalisasi: {:.4f}'.format(test_mae_denorm))

"""##**TARGET 1**"""

import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import PoissonRegressor

data_pusat = df[['Week','Tavg','RH_avg','RR','DBD_lag','Score','Target_1']]
data_pusat = data_pusat.set_index('Week')
data = data_pusat
n_features = data.shape[1]
n_outputs = 1

# Split data
train_data = data.iloc[:int(0.8*len(data)), :]    # Menentukan banyaknya data train yaitu sebesar 80% data
test_data = data.iloc[int(0.8*len(data)):, :]
train_data_raw, test_data_raw = train_data.copy(), test_data.copy()

#Scaling data
scaler = MinMaxScaler()
scaler = scaler.fit(train_data)
train_data = scaler.transform(train_data)
test_data = scaler.transform(test_data)

# Define a function to prepare the data
def prepare_data(data, n_steps):
    X, y = [], []
    for i in range(len(data)-n_steps):
        X.append(data[i:(i+n_steps), :])
        y.append(data[(i+n_steps), -1])
    return np.array(X), np.array(y)

# Define the number of time steps to look back
n_steps = 1

# Prepare the training data
X_train, y_train = prepare_data(train_data, n_steps)
X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).float()

# Prepare the testing data
X_test, y_test = prepare_data(test_data, n_steps)
X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).float()

# Define the LSTM model
class AttentionLSTM(nn.Module):
    def __init__(self, n_features, n_outputs, n_hidden=512, n_layers=3, attention_size=128):
        super().__init__()
        self.n_features = n_features
        self.n_hidden = n_hidden
        self.n_layers = n_layers
        self.n_outputs = n_outputs
        self.attention_size = attention_size

        self.lstm = nn.LSTM(input_size=n_features, hidden_size=n_hidden, num_layers=n_layers, batch_first=True)

        self.attention = nn.Linear(n_hidden, attention_size)
        self.projection = nn.Linear(attention_size, 1, bias=False)
        self.regressor = nn.Linear(n_hidden, n_outputs)

    def forward(self, x):
        output, hidden = self.lstm(x)

        attention_input = torch.tanh(self.attention(output))
        attention_scores = self.projection(attention_input)
        attention_weights = F.softmax(attention_scores, dim=1)

        context_vector = torch.bmm(output.transpose(1, 2), attention_weights).squeeze(2)
        output = self.regressor(context_vector)

        return output

# Initialize the model and define the optimizer and loss function
model = AttentionLSTM(n_features, n_outputs)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Train the model
train_loss = []
train_rmse = []
train_mae = []
n_epochs = 100
for epoch in range(n_epochs):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train.unsqueeze(1))
    loss.backward()
    optimizer.step()
    rmse = np.sqrt(mean_squared_error(y_train, outputs.detach().numpy()))
    mae = mean_absolute_error(y_train, outputs.detach().numpy())
    train_loss.append(loss.item())
    train_rmse.append(rmse)
    train_mae.append(mae)
    if epoch % 10 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}, RMSE: {:4f}'.format(epoch+1, n_epochs, loss.item(), rmse))

# Evaluate the model on the training data
with torch.no_grad():
    train_outputs = model(X_train)
    train_loss = criterion(train_outputs, y_train.unsqueeze(1))
    print('Train Loss: {:.4f}'.format(train_loss.item()))
    train_rmse = np.sqrt(mean_squared_error(y_train, train_outputs.numpy()))
    train_mae = mean_absolute_error(y_train, train_outputs.numpy())
    print('Train RMSE: {:.4f}'.format(train_rmse))
    print('Train MAE: {:.4f}'.format(train_mae))

# Evaluate the model on the testing data
with torch.no_grad():
    test_outputs = model(X_test)
    test_loss = criterion(test_outputs, y_test.unsqueeze(1))
    print('Test Loss: {:.4f}'.format(test_loss.item()))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_outputs.numpy()))
    test_mae = mean_absolute_error(y_test, test_outputs.numpy())
    print('Test RMSE: {:.4f}'.format(test_rmse))
    print('Test MAE: {:.4f}'.format(test_mae))

att_test_pred_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], test_outputs.detach().numpy().reshape(-1, 1))))[:,-1]
att_test_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], y_test.reshape(-1,1))))[:,-1]
test_rmse_denorm = np.sqrt(mean_squared_error(att_test_pred_rescaled, att_test_rescaled))
test_mae_denorm = mean_absolute_error(att_test_pred_rescaled, att_test_rescaled)
print('Test RMSE Denormalisasi: {:.4f}'.format(test_rmse_denorm))
print('Test MAE Denormalisasi: {:.4f}'.format(test_mae_denorm))

"""## **TARGET 2**"""

import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import PoissonRegressor

data_pusat = df[['Week','Tavg','RH_avg','RR','DBD_lag','Score','Target_2']]
data_pusat = data_pusat.set_index('Week')
data = data_pusat
n_features = data.shape[1]
n_outputs = 1

# Split data
train_data = data.iloc[:int(0.8*len(data)), :]    # Menentukan banyaknya data train yaitu sebesar 80% data
test_data = data.iloc[int(0.8*len(data)):, :]
train_data_raw, test_data_raw = train_data.copy(), test_data.copy()

#Scaling data
scaler = MinMaxScaler()
scaler = scaler.fit(train_data)
train_data = scaler.transform(train_data)
test_data = scaler.transform(test_data)

# Define a function to prepare the data
def prepare_data(data, n_steps):
    X, y = [], []
    for i in range(len(data)-n_steps):
        X.append(data[i:(i+n_steps), :])
        y.append(data[(i+n_steps), -1])
    return np.array(X), np.array(y)

# Define the number of time steps to look back
n_steps = 1

# Prepare the training data
X_train, y_train = prepare_data(train_data, n_steps)
X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).float()

# Prepare the testing data
X_test, y_test = prepare_data(test_data, n_steps)
X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).float()

# Define the LSTM model
class AttentionLSTM(nn.Module):
    def __init__(self, n_features, n_outputs, n_hidden=512, n_layers=3, attention_size=128):
        super().__init__()
        self.n_features = n_features
        self.n_hidden = n_hidden
        self.n_layers = n_layers
        self.n_outputs = n_outputs
        self.attention_size = attention_size

        self.lstm = nn.LSTM(input_size=n_features, hidden_size=n_hidden, num_layers=n_layers, batch_first=True)

        self.attention = nn.Linear(n_hidden, attention_size)
        self.projection = nn.Linear(attention_size, 1, bias=False)
        self.regressor = nn.Linear(n_hidden, n_outputs)

    def forward(self, x):
        output, hidden = self.lstm(x)

        attention_input = torch.tanh(self.attention(output))
        attention_scores = self.projection(attention_input)
        attention_weights = F.softmax(attention_scores, dim=1)

        context_vector = torch.bmm(output.transpose(1, 2), attention_weights).squeeze(2)
        output = self.regressor(context_vector)

        return output

# Initialize the model and define the optimizer and loss function
model = AttentionLSTM(n_features, n_outputs)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Train the model
train_loss = []
train_rmse = []
train_mae = []
n_epochs = 100
for epoch in range(n_epochs):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train.unsqueeze(1))
    loss.backward()
    optimizer.step()
    rmse = np.sqrt(mean_squared_error(y_train, outputs.detach().numpy()))
    mae = mean_absolute_error(y_train, outputs.detach().numpy())
    train_loss.append(loss.item())
    train_rmse.append(rmse)
    train_mae.append(mae)
    if epoch % 10 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}, RMSE: {:4f}'.format(epoch+1, n_epochs, loss.item(), rmse))

# Evaluate the model on the training data
with torch.no_grad():
    train_outputs = model(X_train)
    train_loss = criterion(train_outputs, y_train.unsqueeze(1))
    print('Train Loss: {:.4f}'.format(train_loss.item()))
    train_rmse = np.sqrt(mean_squared_error(y_train, train_outputs.numpy()))
    train_mae = mean_absolute_error(y_train, train_outputs.numpy())
    print('Train RMSE: {:.4f}'.format(train_rmse))
    print('Train MAE: {:.4f}'.format(train_mae))

# Evaluate the model on the testing data
with torch.no_grad():
    test_outputs = model(X_test)
    test_loss = criterion(test_outputs, y_test.unsqueeze(1))
    print('Test Loss: {:.4f}'.format(test_loss.item()))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_outputs.numpy()))
    test_mae = mean_absolute_error(y_test, test_outputs.numpy())
    print('Test RMSE: {:.4f}'.format(test_rmse))
    print('Test MAE: {:.4f}'.format(test_mae))

att_test_pred_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], test_outputs.detach().numpy().reshape(-1, 1))))[:,-1]
att_test_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], y_test.reshape(-1,1))))[:,-1]
test_rmse_denorm = np.sqrt(mean_squared_error(att_test_pred_rescaled, att_test_rescaled))
test_mae_denorm = mean_absolute_error(att_test_pred_rescaled, att_test_rescaled)
print('Test RMSE Denormalisasi: {:.4f}'.format(test_rmse_denorm))
print('Test MAE Denormalisasi: {:.4f}'.format(test_mae_denorm))

"""## **SUM OF TARGET 1 & TARGET 2**"""

import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import PoissonRegressor

data_pusat = df[['Week','Tavg','RH_avg','RR','DBD_lag','Score','Sum_Targets']]
data_pusat = data_pusat.set_index('Week')
data = data_pusat
n_features = data.shape[1]
n_outputs = 1

# Split data
train_data = data.iloc[:int(0.8*len(data)), :]    # Menentukan banyaknya data train yaitu sebesar 80% data
test_data = data.iloc[int(0.8*len(data)):, :]
train_data_raw, test_data_raw = train_data.copy(), test_data.copy()

#Scaling data
scaler = MinMaxScaler()
scaler = scaler.fit(train_data)
train_data = scaler.transform(train_data)
test_data = scaler.transform(test_data)

# Define a function to prepare the data
def prepare_data(data, n_steps):
    X, y = [], []
    for i in range(len(data)-n_steps):
        X.append(data[i:(i+n_steps), :])
        y.append(data[(i+n_steps), -1])
    return np.array(X), np.array(y)

# Define the number of time steps to look back
n_steps = 1

# Prepare the training data
X_train, y_train = prepare_data(train_data, n_steps)
X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).float()

# Prepare the testing data
X_test, y_test = prepare_data(test_data, n_steps)
X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).float()

# Define the LSTM model
class AttentionLSTM(nn.Module):
    def __init__(self, n_features, n_outputs, n_hidden=512, n_layers=3, attention_size=128):
        super().__init__()
        self.n_features = n_features
        self.n_hidden = n_hidden
        self.n_layers = n_layers
        self.n_outputs = n_outputs
        self.attention_size = attention_size

        self.lstm = nn.LSTM(input_size=n_features, hidden_size=n_hidden, num_layers=n_layers, batch_first=True)

        self.attention = nn.Linear(n_hidden, attention_size)
        self.projection = nn.Linear(attention_size, 1, bias=False)
        self.regressor = nn.Linear(n_hidden, n_outputs)

    def forward(self, x):
        output, hidden = self.lstm(x)

        attention_input = torch.tanh(self.attention(output))
        attention_scores = self.projection(attention_input)
        attention_weights = F.softmax(attention_scores, dim=1)

        context_vector = torch.bmm(output.transpose(1, 2), attention_weights).squeeze(2)
        output = self.regressor(context_vector)

        return output

# Initialize the model and define the optimizer and loss function
model = AttentionLSTM(n_features, n_outputs)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Train the model
train_loss = []
train_rmse = []
train_mae = []
n_epochs = 100
for epoch in range(n_epochs):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train.unsqueeze(1))
    loss.backward()
    optimizer.step()
    rmse = np.sqrt(mean_squared_error(y_train, outputs.detach().numpy()))
    mae = mean_absolute_error(y_train, outputs.detach().numpy())
    train_loss.append(loss.item())
    train_rmse.append(rmse)
    train_mae.append(mae)
    if epoch % 10 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}, RMSE: {:4f}'.format(epoch+1, n_epochs, loss.item(), rmse))

# Evaluate the model on the training data
with torch.no_grad():
    train_outputs = model(X_train)
    train_loss = criterion(train_outputs, y_train.unsqueeze(1))
    print('Train Loss: {:.4f}'.format(train_loss.item()))
    train_rmse = np.sqrt(mean_squared_error(y_train, train_outputs.numpy()))
    train_mae = mean_absolute_error(y_train, train_outputs.numpy())
    print('Train RMSE: {:.4f}'.format(train_rmse))
    print('Train MAE: {:.4f}'.format(train_mae))

# Evaluate the model on the testing data
with torch.no_grad():
    test_outputs = model(X_test)
    test_loss = criterion(test_outputs, y_test.unsqueeze(1))
    print('Test Loss: {:.4f}'.format(test_loss.item()))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_outputs.numpy()))
    test_mae = mean_absolute_error(y_test, test_outputs.numpy())
    print('Test RMSE: {:.4f}'.format(test_rmse))
    print('Test MAE: {:.4f}'.format(test_mae))

att_test_pred_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], test_outputs.detach().numpy().reshape(-1, 1))))[:,-1]
att_test_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], y_test.reshape(-1,1))))[:,-1]
test_rmse_denorm = np.sqrt(mean_squared_error(att_test_pred_rescaled, att_test_rescaled))
test_mae_denorm = mean_absolute_error(att_test_pred_rescaled, att_test_rescaled)
print('Test RMSE Denormalisasi: {:.4f}'.format(test_rmse_denorm))
print('Test MAE Denormalisasi: {:.4f}'.format(test_mae_denorm))

"""# **INTERPOLATION FOR WEEK 761-764**"""

df.info()
df.head()

df_dummy = df.copy()

df_dummy = df_dummy[['Week','Tavg','RH_avg','RR', 'DBD', 'DBD_lag','Score']]

df_dummy[['DBD','DBD_lag']] = df_dummy[['DBD','DBD_lag']].astype('float')

# Add dummy rows with missing values for Week761 to Week764
dummy_rows_data = [[761, None, None, None, None, None, None],
                   [762, None, None, None, None, None, None],
                   [763, None, None, None, None, None, None],
                   [764, None, None, None, None, None, None],
                   [765, None, None, None, None, None, None]]

dummy_rows = pd.DataFrame(dummy_rows_data, columns=df_dummy.columns)

# Append the dummy rows to the original DataFrame
df_dummy = df_dummy.append(dummy_rows, ignore_index=True)

df_dummy.tail()

df_interpolate = df_dummy.copy()

# Select columns for interpolation
columns_to_interpolate = ['Tavg', 'RH_avg', 'RR', 'DBD', 'Score']

# Interpolate specified columns using spline method
df_interpolate[columns_to_interpolate] = df_interpolate[columns_to_interpolate].interpolate(method='spline', order=2)

# Ensure 'Score' values do not exceed 100
df_interpolate['Score'] = df_interpolate['Score'].apply(lambda x: min(x, 100) if pd.notnull(x) else x)

df_interpolate.tail(6)

#perform ceiling function for DBD column for week 761 to 764

df_interpolate['DBD'] = np.ceil(df_interpolate['DBD'])

df_interpolate.tail(6)

# Identify the missing values in the column
missing_values_indices = df_interpolate['DBD_lag'].isnull()

# Define values to fill for each missing value
fill_values = [151.0, 124.0, 122.0, 145.0, 194.0]

# Filter the index based on missing values
missing_values_index = missing_values_indices[missing_values_indices].index

# Fill missing values in the 'DBD_lag' column with different values
df_interpolate['DBD_lag'].fillna(value=pd.Series(fill_values, index=missing_values_index), inplace=True)

df_interpolate.tail(6)

"""# **PREDICTION FOR WEEK 761-764**"""

df_predict = df_interpolate.copy()

# Calculate Target_1 as the rolling average
df_predict['Target_1'] = df_predict['DBD'].rolling(window=3, min_periods=1).mean()

# Calculate Target_2 as the difference between consecutive weeks
df_predict['Target_2'] = df_predict['DBD'].diff()

# Replace NaN values in Target_2 with 0.1
df_predict['Target_2'].fillna(1, inplace=True)

# Calculate Sum_Targets as the sum of Target_1 and Target_2
df_predict['Sum_Targets'] = df_predict['Target_1'] + df_predict['Target_2']

df_predict.tail()

import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import PoissonRegressor

data_pusat = df_predict[['Week','Tavg','RH_avg','RR','DBD_lag','Score','Sum_Targets']]
data_pusat = data_pusat.set_index('Week')
data = data_pusat
n_features = data.shape[1]
n_outputs = 1

# Split data
train_data = data.iloc[:int(0.8*len(data)), :]    # Menentukan banyaknya data train yaitu sebesar 80% data
test_data = data.iloc[int(0.8*len(data)):, :]
train_data_raw, test_data_raw = train_data.copy(), test_data.copy()

#Scaling data
scaler = MinMaxScaler()
scaler = scaler.fit(train_data)
train_data = scaler.transform(train_data)
test_data = scaler.transform(test_data)

# Define a function to prepare the data
def prepare_data(data, n_steps):
    X, y = [], []
    for i in range(len(data)-n_steps):
        X.append(data[i:(i+n_steps), :])
        y.append(data[(i+n_steps), -1])
    return np.array(X), np.array(y)

# Define the number of time steps to look back
n_steps = 1

# Prepare the training data
X_train, y_train = prepare_data(train_data, n_steps)
X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).float()

# Prepare the testing data
X_test, y_test = prepare_data(test_data, n_steps)
X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).float()

# Define the LSTM model
class AttentionLSTM(nn.Module):
    def __init__(self, n_features, n_outputs, n_hidden=512, n_layers=3, attention_size=128):
        super().__init__()
        self.n_features = n_features
        self.n_hidden = n_hidden
        self.n_layers = n_layers
        self.n_outputs = n_outputs
        self.attention_size = attention_size

        self.lstm = nn.LSTM(input_size=n_features, hidden_size=n_hidden, num_layers=n_layers, batch_first=True)

        self.attention = nn.Linear(n_hidden, attention_size)
        self.projection = nn.Linear(attention_size, 1, bias=False)
        self.regressor = nn.Linear(n_hidden, n_outputs)

    def forward(self, x):
        output, hidden = self.lstm(x)

        attention_input = torch.tanh(self.attention(output))
        attention_scores = self.projection(attention_input)
        attention_weights = F.softmax(attention_scores, dim=1)

        context_vector = torch.bmm(output.transpose(1, 2), attention_weights).squeeze(2)
        output = self.regressor(context_vector)

        return output

# Initialize the model and define the optimizer and loss function
model = AttentionLSTM(n_features, n_outputs)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# Train the model
train_loss = []
train_rmse = []
train_mae = []
n_epochs = 100
for epoch in range(n_epochs):
    optimizer.zero_grad()
    outputs = model(X_train)
    loss = criterion(outputs, y_train.unsqueeze(1))
    loss.backward()
    optimizer.step()
    rmse = np.sqrt(mean_squared_error(y_train, outputs.detach().numpy()))
    mae = mean_absolute_error(y_train, outputs.detach().numpy())
    train_loss.append(loss.item())
    train_rmse.append(rmse)
    train_mae.append(mae)
    if epoch % 10 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}, RMSE: {:4f}'.format(epoch+1, n_epochs, loss.item(), rmse))

# Evaluate the model on the training data
with torch.no_grad():
    train_outputs = model(X_train)
    train_loss = criterion(train_outputs, y_train.unsqueeze(1))
    print('Train Loss: {:.4f}'.format(train_loss.item()))
    train_rmse = np.sqrt(mean_squared_error(y_train, train_outputs.numpy()))
    train_mae = mean_absolute_error(y_train, train_outputs.numpy())
    print('Train RMSE: {:.4f}'.format(train_rmse))
    print('Train MAE: {:.4f}'.format(train_mae))

# Evaluate the model on the testing data
with torch.no_grad():
    test_outputs = model(X_test)
    test_loss = criterion(test_outputs, y_test.unsqueeze(1))
    print('Test Loss: {:.4f}'.format(test_loss.item()))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_outputs.numpy()))
    test_mae = mean_absolute_error(y_test, test_outputs.numpy())
    print('Test RMSE: {:.4f}'.format(test_rmse))
    print('Test MAE: {:.4f}'.format(test_mae))

att_test_pred_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], test_outputs.detach().numpy().reshape(-1, 1))))[:,-1]
att_test_rescaled = scaler.inverse_transform(np.hstack((X_test[:, -1, :-1], y_test.reshape(-1,1))))[:,-1]
test_rmse_denorm = np.sqrt(mean_squared_error(att_test_pred_rescaled, att_test_rescaled))
test_mae_denorm = mean_absolute_error(att_test_pred_rescaled, att_test_rescaled)
print('Test RMSE Denormalisasi: {:.4f}'.format(test_rmse_denorm))
print('Test MAE Denormalisasi: {:.4f}'.format(test_mae_denorm))

# Apply the same scaling as used for training data
data_pusat_scaled = scaler.transform(data_pusat)

# Prepare the new data for prediction
X_predict, _ = prepare_data(data_pusat_scaled, n_steps)

# Convert to PyTorch tensors
X_predict_tensor = torch.from_numpy(X_predict).float()

# Make predictions
with torch.no_grad():
    model.eval()  # Set the model to evaluation mode
    predictions = model(X_predict_tensor)

# Inverse transform predictions
predictions_denorm = scaler.inverse_transform(np.hstack((X_predict[:, -1, :-1], predictions.numpy().reshape(-1, 1))))[:, -1]

# Reset the index of df_predict
df_predict.reset_index(drop=True, inplace=True)

# Exclude the last row from df_predict
df_predict = df_predict.iloc[:-1]

# Add predictions to the DataFrame
df_predict['Prediction'] = predictions_denorm

# Display the DataFrame with predictions
print(df_predict[['Sum_Targets', 'Prediction']])

df_predict.info()
df_predict.tail()

df_final = df_predict.copy()

# Define the threshold for DHF outbreak
outbreak_threshold = 2

# Initialize a list to store outbreak status
outbreak_status = []

# Iterate through the DataFrame to detect outbreaks and set the status
for i in range(1, len(df_final)):
    current_week_cases = df_final.loc[i, 'DBD']
    prev_week_cases = df_final.loc[i - 1, 'DBD']

    if current_week_cases >= prev_week_cases * outbreak_threshold:
        outbreak_status.append('Yes')
    else:
        outbreak_status.append('No')

# Add the outbreak_status column to the DataFrame
df_final['Outbreak_Status'] = ['No'] + outbreak_status

df_final.info()
df_final.tail()

df_final = df_final[['Week','Tavg','RH_avg','RR','DBD','Score','Sum_Targets', 'Prediction', 'Outbreak_Status']]

df_final.head()

(abs(df_final['Prediction']-df_final['DBD'])).mean()

df_final.Outbreak_Status.value_counts()

import matplotlib.pyplot as plt


# Plotting the time series for DBD and prediction
plt.figure(figsize=(45, 20))

# Plot DBD cases
plt.plot(df_final['Week'], df_final['DBD'], label='DBD Cases', marker='o', linestyle=':', linewidth=4)

# Plot sum_target
plt.plot(df_final['Week'], df_final['Prediction'], label='Prediction', marker='o', linestyle='-', linewidth=4)

# Add labels and title
plt.xlabel('Week')
plt.ylabel('Number of Cases')
plt.title('Time Series of DBD Cases and Prediction in DKI Jakarta')
plt.legend()  # Show legend

# Set legend font size
plt.legend(fontsize='large')

# Show the plot
plt.show()

"""# **SAVE FINAL DATA**"""

df_final.info()
df_final.tail()

# prompt: i want to save df_final dataframe into my local disk without index

df_final.to_csv('df_final_no_index.csv', index=False)