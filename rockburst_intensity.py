# -*- coding: utf-8 -*-
"""Rockburst Intensity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rAudYFKDEN4yjL66NbmsIhix7xjeARqS

**ADHITYA DWI NUGRAHA**

**You can access the dataset through this link**

https://docs.google.com/spreadsheets/d/1ih8Fe2BS848FyKRoZhbgK-jLMMYXypyH/edit?usp=drive_link&ouid=101190233143685390070&rtpof=true&sd=true

**Objective of this program is to compare model performance in classifying rockburst events and intensity in underground**

# **START**
"""

#Import library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE

#Connecting to google drive
from google.colab import drive
drive.mount('/content/drive')

#Path to saved the dataset
path = "/content/drive/MyDrive/Skripsweet/Dataset.xlsx"

#Load data from excel
data = pd.read_excel(path, sheet_name='Sheet1')

#Rename columns
data.rename(columns={'Rock type':'rock_type','σθ / Mpa':'MTS','σc / Mpa':'UCS','σt / MPa':'UTS',
                     'σθ/σc':'SC','σc/σt':'RBC','Wet':'ESI','Rockburst intensity':'Intensity'
                     }, inplace=True)
print(data.columns)

#Drop column cite by
data.drop(['rock_type','Cite by'], axis=1, inplace=True)

print(data.columns)

vc = data['Intensity'].value_counts()
colr = plt.cm.get_cmap('tab10', len(vc))

plt.bar(vc.index, vc.values, color=colr(np.arange(len(vc))))

plt.xlabel('Rockburst Intensity')
plt.ylabel('Count')
plt.title('Class Composition')

plt.show()

data.head()

# drop rows with missing values in column intensity
data.dropna(subset=['Intensity'], inplace=True)

"""# **PRE-PROCESSING**"""

#data imputation
data_imputed = data.copy()

cols_with_missing = data_imputed.columns[data_imputed.isna().any()].tolist()

# Loop over each column with missing data and perform median imputation
for col in cols_with_missing:
    mean_value = np.mean(data_imputed[col].dropna())
    data_imputed[col].fillna(mean_value, inplace=True)

def rb_occurence(df):
  if df['Intensity'] == 'Moderate':
    return 'Yes'
  if df['Intensity'] == 'Strong':
    return 'Yes'
  if df['Intensity'] == 'Weak':
    return 'Yes'
  else:
    return 'No'

data_imputed['Occurence'] = data_imputed.apply(rb_occurence, axis=1)

data_imputed.head(8)

# List of columns to check for outliers
cols_to_check = ['MTS','UCS','UTS','SC','RBC','ESI']

# Create box plots for each column
for col in cols_to_check:
    # Calculate the IQR and upper and lower bounds for the column
    q1 = data_imputed[col].quantile(0.25)
    q3 = data_imputed[col].quantile(0.75)
    iqr = q3 - q1
    upper_bound = q3 + (1.5 * iqr)
    lower_bound = q1 - (1.5 * iqr)

    # Highlight the outliers
    outliers = data_imputed[(data_imputed[col] > upper_bound) | (data_imputed[col] < lower_bound)]
    num_outliers = len(outliers)
    print(f"Number of outliers in columns {col}: {num_outliers}")
    print(f"Rows with outliers : {outliers}")

data_clean = data_imputed.copy()

# define function to remove outliers using IQR
def remove_outliers(df, columns):
    result = df.copy()
    for feature_name in columns:
        Q1 = df[feature_name].quantile(0.25)
        Q3 = df[feature_name].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        result = result[(result[feature_name] >= lower_bound) & (result[feature_name] <= upper_bound)]
    return result

# apply outlier removal to selected columns
data_clean = remove_outliers(data_clean, ['MTS','UTS', 'UCS','SC','RBC','ESI'])

print(data_clean)

data_transformed = data_clean.copy()

# List of columns to normalize
cols_to_normalize = ['MTS', 'UCS', 'UTS', 'SC', 'RBC', 'ESI']

# Apply min-max normalization to each column
for col in cols_to_normalize:
    data_transformed[col] = (data_transformed[col] - data_transformed[col].min()) / (data_transformed[col].max() - data_transformed[col].min())

# Print the normalized DataFrame
print(data_transformed)

# Convert the data to pandas DataFrames for easier plotting
feature_columns = ['MTS', 'UCS', 'UTS', 'SC', 'RBC', 'ESI']
df_before = pd.DataFrame(data_clean, columns=feature_columns)
df_after = pd.DataFrame(data_transformed, columns=feature_columns)

# Create histograms for each feature before and after normalization
plt.figure(figsize=(16, 8))
for i, feature in enumerate(feature_columns):
    plt.subplot(2, 3, i + 1)
    plt.hist(df_before[feature], alpha=0.5, label='Before Normalization', bins=20)
    plt.hist(df_after[feature], alpha=0.5, label='After Normalization', bins=20)
    plt.title(f'{feature} Distribution')
    plt.legend()

plt.tight_layout()
plt.show()

data_model2 = data_transformed.loc[data_transformed['Occurence'] !='No']

X1 = data_transformed.drop(['Intensity','Occurence'], axis=1)
Y1 = data_transformed['Occurence']

X2 = data_model2.drop(['Intensity','Occurence'], axis=1)
Y2 = data_model2['Intensity']

from sklearn import svm
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score
import xgboost as xgb
from xgboost import XGBClassifier
import time

# Mapping dictionary
mapping = {'Yes': 1, 'No': 0}
mapping2 = {'Weak': 0, 'Moderate': 1, 'Strong': 2}

# Apply mapping to encode the target column
Y1 = [mapping[label] for label in Y1]
Y2 = [mapping2[label] for label in Y2]

"""# **MODEL 1 - Rockburst Events Classification**

## **GWO-SVM**
"""

# split the dataset into training and testing sets
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, Y1, test_size=0.2, random_state=42)

# Apply SMOTE only to the training set
smote = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42, n_jobs=None)
X_train_smote, y_train_smote = smote.fit_resample(X1_train, y1_train)

np.random.seed(42)

# Define the GWO class
class GWO:
    def __init__(self, n_iter, n_pop, n_dim, lb, ub, obj_func):
        self.n_iter = n_iter
        self.n_pop = n_pop
        self.n_dim = n_dim
        self.lb = lb
        self.ub = ub
        self.obj_func = obj_func
        self.alpha_pos = None
        self.beta_pos = None
        self.delta_pos = None
        self.alpha_score = np.inf
        self.beta_score = np.inf
        self.delta_score = np.inf
        self.pos = np.random.uniform(low=self.lb, high=self.ub, size=(self.n_pop, self.n_dim))

    def run(self):
        for t in range(self.n_iter):
            for i in range(self.n_pop):
                # Boundary checking
                self.pos[i] = np.clip(self.pos[i], self.lb, self.ub)

                # Calculate objective function value
                fitness = self.obj_func(self.pos[i])

                # Update alpha, beta, and delta
                if fitness < self.alpha_score:
                    self.alpha_score = fitness
                    self.alpha_pos = self.pos[i]
                elif fitness < self.beta_score:
                    self.beta_score = fitness
                    self.beta_pos = self.pos[i]
                elif fitness < self.delta_score:
                    self.delta_score = fitness
                    self.delta_pos = self.pos[i]

            # Update a, A, C, and D
            a = 2 - 2 * (t / self.n_iter)
            A = 2 * a * np.random.rand(self.n_dim) - a
            C = 2 * np.random.rand(self.n_dim)
            D = np.abs(C * self.alpha_pos - self.pos)

            # Update position
            self.pos = self.alpha_pos - A * D

        return self.alpha_pos

# Define the SVM objective function
def svm_obj_func(x):
    gamma, C = x
    clf = SVC(C=C, gamma=gamma, kernel='rbf')
    clf.fit(X_train_smote, y_train_smote)
    y_pred = clf.predict(X1_test)
    return 1 - accuracy_score(y1_test, y_pred)

# Define the GWO-SVM model
n_iter = 20
n_pop = 20
n_dim = 2
lb = [0.1, 1]
ub = [25, 50]

# Start the timer
start_time = time.time()

gwo_svm = GWO(n_iter=n_iter, n_pop=n_pop, n_dim=n_dim, lb=lb, ub=ub, obj_func=svm_obj_func)

# Train the model
best_params = gwo_svm.run()

# Fit the SVM model with the best hyperparameters
clf = SVC(C=best_params[1], gamma=best_params[0], kernel='rbf')
clf.fit(X_train_smote, y_train_smote)

# Evaluate the model on test data
y_pred = clf.predict(X1_test)
accuracy = accuracy_score(y1_test, y_pred)
precision = precision_score(y1_test, y_pred, average='macro')
recall = recall_score(y1_test, y_pred, average='macro')
f1 = f1_score(y1_test, y_pred, average='macro')
cm = confusion_matrix(y1_test, y_pred)

# Evaluate the model on training data
y_pred_train = clf.predict(X_train_smote)
accuracy_train = accuracy_score(y_train_smote, y_pred_train)
precision_train = precision_score(y_train_smote, y_pred_train, average='macro')
recall_train = recall_score(y_train_smote, y_pred_train, average='macro')
f1_train = f1_score(y_train_smote, y_pred_train, average='macro')
cm_train = confusion_matrix(y_train_smote, y_pred_train)

# Calculate the running time
running_time = time.time() - start_time

# Print metrics for test data
print(classification_report(y1_test, y_pred))
print("Accuracy: {:.4f}%".format(accuracy * 100))
print("Precision: {:.4f}%".format(precision * 100))
print("Recall: {:.4f}%".format(recall * 100))
print("F1-Score: {:.4f}%".format(f1 * 100))
print("Confusion Matrix:\n", cm)

# Print metrics for training data
print("\nTraining Data:")
print(classification_report(y_train_smote, y_pred_train))
print("Accuracy: {:.4f}%".format(accuracy_train * 100))
print("Precision: {:.4f}%".format(precision_train * 100))
print("Recall: {:.4f}%".format(recall_train * 100))
print("F1-Score: {:.4f}%".format(f1_train * 100))
print("Confusion Matrix:\n", cm_train)

print("Running Time: {:.2f} seconds".format(running_time))

"""## **XGBoost**"""

np.random.seed(42)

# Define the XGBoost parameters
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'logloss',
    'eta': 1.5,
    'max_depth': 6,
    'min_child_weight': 2,
    'gamma': 0,
    'subsample': 0.5,
    'colsample_bytree': 0.5,
    'alpha': 1,
    'lambda': 1,
    'seed': 42
}

# Convert the data into XGBoost DMatrix format
dtrain = xgb.DMatrix(X_train_smote, label=y_train_smote)
dtest = xgb.DMatrix(X1_test, label=y1_test)

start_time = time.time()  # Start measuring the running time

# Train the XGBoost model
num_round = 100
bst = xgb.train(params, dtrain, num_round)

# Predict the labels of the testing set
y_pred = bst.predict(dtest)
y_pred = [int(round(x)) for x in y_pred]

# Compute the accuracy of the classifier
accuracy = accuracy_score(y1_test, y_pred)
precision = precision_score(y1_test, y_pred, average='macro')
recall = recall_score(y1_test, y_pred, average='macro')
f1 = f1_score(y1_test, y_pred, average='macro')
cm = confusion_matrix(y1_test, y_pred)

# Predict the labels of the training set
y_pred_train = bst.predict(dtrain)
y_pred_train = [int(round(x)) for x in y_pred_train]

# Compute the metrics for training data
accuracy_train = accuracy_score(y_train_smote, y_pred_train)
precision_train = precision_score(y_train_smote, y_pred_train, average='macro')
recall_train = recall_score(y_train_smote, y_pred_train, average='macro')
f1_train = f1_score(y_train_smote, y_pred_train, average='macro')
cm_train = confusion_matrix(y_train_smote, y_pred_train)

# Print metrics for test data
print(classification_report(y1_test, y_pred))
print("Accuracy: {:.4f}%".format(accuracy * 100))
print("Precision: {:.4f}%".format(precision * 100))
print("Recall: {:.4f}%".format(recall * 100))
print("F1-Score: {:.4f}%".format(f1 * 100))
print("Confusion Matrix:\n", cm)

# Print metrics for training data
print("\nTraining Data:")
print(classification_report(y_train_smote, y_pred_train))
print("Accuracy: {:.4f}%".format(accuracy_train * 100))
print("Precision: {:.4f}%".format(precision_train * 100))
print("Recall: {:.4f}%".format(recall_train * 100))
print("F1-Score: {:.4f}%".format(f1_train * 100))
print("Confusion Matrix:\n", cm_train)

# Plot colorful confusion matrix for test data
plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix - Test Data')
plt.show()

# Plot colorful confusion matrix for training data
plt.figure(figsize=(4, 3))
sns.heatmap(cm_train, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix - Training Data')
plt.show()

end_time = time.time()  # Stop measuring the running time
running_time = end_time - start_time
print("Running Time: {:.4f} seconds".format(running_time))

"""# **MODEL 2 - Rockburst Intensity Classification**

## **GWO-SVM**
"""

# split the dataset into training and testing sets
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, Y2, test_size=0.1, random_state=42)

# Apply SMOTE only to the training set
smote = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42, n_jobs=None)
X2_train_smote, y2_train_smote = smote.fit_resample(X2_train, y2_train)

np.random.seed(42)

# Define the GWO class
class GWO:
    def __init__(self, n_iter, n_pop, n_dim, lb, ub, obj_func):
        self.n_iter = n_iter
        self.n_pop = n_pop
        self.n_dim = n_dim
        self.lb = lb
        self.ub = ub
        self.obj_func = obj_func
        self.alpha_pos = None
        self.beta_pos = None
        self.delta_pos = None
        self.alpha_score = np.inf
        self.beta_score = np.inf
        self.delta_score = np.inf
        self.pos = np.random.uniform(low=self.lb, high=self.ub, size=(self.n_pop, self.n_dim))

    def run(self):
        for t in range(self.n_iter):
            for i in range(self.n_pop):
                # Boundary checking
                self.pos[i] = np.clip(self.pos[i], self.lb, self.ub)

                # Calculate objective function value
                fitness = self.obj_func(self.pos[i])

                # Update alpha, beta, and delta
                if fitness < self.alpha_score:
                    self.alpha_score = fitness
                    self.alpha_pos = self.pos[i]
                elif fitness < self.beta_score:
                    self.beta_score = fitness
                    self.beta_pos = self.pos[i]
                elif fitness < self.delta_score:
                    self.delta_score = fitness
                    self.delta_pos = self.pos[i]

            # Update a, A, C, and D
            a = 2 - 2 * (t / self.n_iter)
            A = 2 * a * np.random.rand(self.n_dim) - a
            C = 2 * np.random.rand(self.n_dim)
            D = np.abs(C * self.alpha_pos - self.pos)

            # Update position
            self.pos = self.alpha_pos - A * D

        return self.alpha_pos

# Define the SVM objective function
def svm_obj_func(x):
    gamma, C = x
    clf = SVC(C=C, gamma=gamma, kernel='rbf')
    clf.fit(X2_train_smote, y2_train_smote)
    y_pred = clf.predict(X2_test)
    return 1 - accuracy_score(y2_test, y_pred)

# Define the GWO-SVM model
n_iter = 20
n_pop = 20
n_dim = 2
lb = [0.1, 1]
ub = [25, 50]

start_time = time.time()

gwo_svm = GWO(n_iter=n_iter, n_pop=n_pop, n_dim=n_dim, lb=lb, ub=ub, obj_func=svm_obj_func)

# Train the model
best_params = gwo_svm.run()

# Fit the SVM model with the best hyperparameters
clf = SVC(C=best_params[1], gamma=best_params[0], kernel='rbf')
clf.fit(X2_train_smote, y2_train_smote)

# Evaluate the model on test data
y_pred = clf.predict(X2_test)
accuracy = accuracy_score(y2_test, y_pred)
precision = precision_score(y2_test, y_pred, average='macro')
recall = recall_score(y2_test, y_pred, average='macro')
f1 = f1_score(y2_test, y_pred, average='macro')
cm = confusion_matrix(y2_test, y_pred)

# Evaluate the model on training data
y_pred_train = clf.predict(X2_train_smote)
accuracy_train = accuracy_score(y2_train_smote, y_pred_train)
precision_train = precision_score(y2_train_smote, y_pred_train, average='macro')
recall_train = recall_score(y2_train_smote, y_pred_train, average='macro')
f1_train = f1_score(y2_train_smote, y_pred_train, average='macro')
cm_train = confusion_matrix(y2_train_smote, y_pred_train)

end_time = time.time()
execution_time = end_time - start_time

# Print metrics for test data
print(classification_report(y2_test, y_pred))
print("Accuracy: {:.4f}%".format(accuracy * 100))
print("Precision: {:.4f}%".format(precision * 100))
print("Recall: {:.4f}%".format(recall * 100))
print("F1-Score: {:.4f}%".format(f1 * 100))
print("Confusion Matrix:\n", cm)

# Print metrics for training data
print("\nTraining Data:")
print(classification_report(y2_train_smote, y_pred_train))
print("Accuracy: {:.4f}%".format(accuracy_train * 100))
print("Precision: {:.4f}%".format(precision_train * 100))
print("Recall: {:.4f}%".format(recall_train * 100))
print("F1-Score: {:.4f}%".format(f1_train * 100))
print("Confusion Matrix:\n", cm_train)

# Plot colorful confusion matrix for test data
plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix - Test Data')
plt.show()

print("Execution Time: {:.2f} seconds".format(execution_time))

"""## **XGBoost**"""

# split the dataset into training and testing sets
X2_train, X2_test, y2_train, y2_test = train_test_split(X2, Y2, test_size=0.1, random_state=42)

# Apply SMOTE only to the training set
smote = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42, n_jobs=None)
X2_train_smote, y2_train_smote = smote.fit_resample(X2_train, y2_train)

# Define the XGBoost parameters

params = {
    'objective': 'multi:softmax',
    'eval_metric': 'mlogloss',
    'num_class': 3,
    'eta': 1.5,
    'max_depth': 6,
    'min_child_weight': 2,
    'gamma': 0,
    'subsample': 0.5,
    'colsample_bytree': 0.5,
    'alpha': 1,
    'lambda': 1,
    'seed': 42
}

# Convert the data into XGBoost DMatrix format
dtrain = xgb.DMatrix(X2_train_smote, label=y2_train_smote)
dtest = xgb.DMatrix(X2_test, label=y2_test)

start_time = time.time()  # Start measuring the running time

# Train the XGBoost model
num_round = 100
bst = xgb.train(params, dtrain, num_round)

# Predict the labels of the testing set
y_pred = bst.predict(dtest)
y_pred = [int(round(x)) for x in y_pred]

# Compute the accuracy of the classifier
accuracy = accuracy_score(y2_test, y_pred)
precision = precision_score(y2_test, y_pred, average='macro')
recall = recall_score(y2_test, y_pred, average='macro')
f1 = f1_score(y2_test, y_pred, average='macro')
cm = confusion_matrix(y2_test, y_pred)

# Predict the labels of the training set
y_pred_train = bst.predict(dtrain)
y_pred_train = [int(round(x)) for x in y_pred_train]

# Compute the metrics for training data
accuracy_train = accuracy_score(y2_train_smote, y_pred_train)
precision_train = precision_score(y2_train_smote, y_pred_train, average='macro')
recall_train = recall_score(y2_train_smote, y_pred_train, average='macro')
f1_train = f1_score(y2_train_smote, y_pred_train, average='macro')
cm_train = confusion_matrix(y2_train_smote, y_pred_train)

#Print metrics for test data
print(classification_report(y2_test, y_pred))
print("Accuracy: {:.4f}%".format(accuracy * 100))
print("Precision: {:.4f}%".format(precision * 100))
print("Recall: {:.4f}%".format(recall * 100))
print("F1-Score: {:.4f}%".format(f1 * 100))
print("Confusion Matrix:\n", cm)

# Print metrics for training data
print("\nTraining Data:")
print(classification_report(y2_train_smote, y_pred_train))
print("Accuracy: {:.4f}%".format(accuracy_train * 100))
print("Precision: {:.4f}%".format(precision_train * 100))
print("Recall: {:.4f}%".format(recall_train * 100))
print("F1-Score: {:.4f}%".format(f1_train * 100))
print("Confusion Matrix:\n", cm_train)

end_time = time.time()  # Stop measuring the running time
running_time = end_time - start_time
print("Running Time: {:.4f} seconds".format(running_time))

"""# **CONCLUSION**

1. In both classifications, GWO-SVM and XGBoost performed very well. For rockburst events classification with training data 80%, both models achieved 97.53% accuracy, 94.44% precision, 98.46% recall, and 96.28% F1-score. Both models were able to predict the yes rockburst class well compared to the other class.

2. For rockburst intensity classification with 90% training data, XGBoost outperforms GWO-SVM in terms of accuracy, recall, F1 score, and running time. Both models were able to predict the weak intensity class well compared to the other classes.
"""