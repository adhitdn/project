# -*- coding: utf-8 -*-
"""Olist Customer 360 View.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c7zZKP0OsTTpGQmZWW_bJnAL9oTwAWz-

**ADHITYA DWI NUGRAHA**

**You can access the dataset through this link**

https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce

# Importing Packages and Datas
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt
from imblearn.over_sampling import SMOTE

np.random.seed(42)

from google.colab import drive
drive.mount('/content/drive/')

cust_path = '/content/drive/MyDrive/Olist/olist_customers_dataset.csv'
geoloc_path = '/content/drive/MyDrive/Olist/olist_geolocation_dataset.csv'
ordit_path = '/content/drive/MyDrive/Olist/olist_order_items_dataset.csv'
orpay_path = '/content/drive/MyDrive/Olist/olist_order_payments_dataset.csv'
orrev_path = '/content/drive/MyDrive/Olist/olist_order_reviews_dataset.csv'
order_path = '/content/drive/MyDrive/Olist/olist_orders_dataset.csv'
prod_path = '/content/drive/MyDrive/Olist/olist_products_dataset.csv'
seller_path = '/content/drive/MyDrive/Olist/olist_sellers_dataset.csv'
procat_path = '/content/drive/MyDrive/Olist/product_category_name_translation.csv'

cust = pd.read_csv(cust_path)
geoloc = pd.read_csv(geoloc_path)
ordit = pd.read_csv(ordit_path)
orpay = pd.read_csv(orpay_path)
orrev = pd.read_csv(orrev_path)
order = pd.read_csv(order_path)
prod = pd.read_csv(prod_path)
seller = pd.read_csv(seller_path)
procat = pd.read_csv(procat_path)

"""# Check Data"""

cust.info()
cust.head()

geoloc.info()
geoloc.head()

ordit.info()
ordit.head()

orpay.info()
orpay.head()

orrev.info()
orrev.head()

order.info()
order.head()

prod.info()
prod.head()

seller.info()
seller.head()

procat.info()
procat.head()

"""# Merging Datas"""

df1 = pd.merge(cust, order, on='customer_id', how='left')
df2 = pd.merge(df1, orrev, on='order_id', how='left')
df3 = pd.merge(df2, orpay, on='order_id', how='left')
df4 = pd.merge(df3, ordit, on='order_id', how='left')
df5 = pd.merge(df4, seller, on='seller_id', how='left')
df6 = pd.merge(df5, prod, on='product_id', how='left')
df6.info()

"""# Data Cleaning"""

main_data = df6.copy()

main_data.info()

main_data.drop(['review_comment_title'], axis=1, inplace=True)
main_data['review_comment_message'].fillna('No Comment', inplace=True)
main_data['review_comment_message'] = main_data['review_comment_message'].apply(lambda x: 'Leave A Comment' if x != 'No Comment' else x)

main_data.review_comment_message.value_counts()

main_data.isnull().sum()

main_data.dropna(inplace=True)

main_data.isnull().sum()

data_eda = main_data.copy()

# Drop rows with order status 'cancelled' from the original DataFrame
data_eda.drop(data_eda[data_eda['order_status'] == 'canceled'].index, inplace=True)

data_eda.info()

data_eda['customer_zip_code_prefix'] = data_eda['customer_zip_code_prefix'].astype('object')
data_eda['seller_zip_code_prefix'] = data_eda['seller_zip_code_prefix'].astype('object')

# Use groupby and first to get the first row for each unique customer ID
unique_customer = data_eda.groupby('customer_id').first().reset_index()

unique_customer.info()

# Use groupby and first to get the first row for each unique customer ID
unique_seller = data_eda.groupby('seller_id').first().reset_index()

unique_seller.info()

"""# **EDA**

## **Order Status Proportion**
"""

data_eda.order_status.value_counts()

"""## **Top 5 Customer State, City, and Zip Code**"""

data_eda[['customer_state','customer_city','seller_state','seller_city']].describe()

# Get the city counts for the top 5 cities
top_states = unique_customer['customer_state'].value_counts().nlargest(5)

# Define alpha values for each bar
alpha_values = [1.0, 0.7, 0.6, 0.5, 0.4]

# Plot the bar chart with the top 5 cities and adjusted alpha values
fig, ax = plt.subplots()

for i, (state, count) in enumerate(top_states.items()):
    ax.bar(state, count, color=['green', 'green', 'green', 'green', 'green'][i], alpha=alpha_values[i], linewidth=5)

# Add labels and title
ax.set_xlabel('State')
ax.set_ylabel('Number of Customer')
ax.set_title('Top 5 States with Highest Customer')

# Display the plot
plt.show()

unique_customer.customer_state.value_counts().nlargest(5)

# Get the city counts for the top 5 cities
top_cities = unique_customer['customer_city'].value_counts().nlargest(5)

# Define alpha values for each bar
alpha_values = [1.0, 0.7, 0.6, 0.5, 0.4]

# Plot the bar chart with the top 5 cities and adjusted alpha values
fig, ax = plt.subplots()

for i, (city, count) in enumerate(top_cities.items()):
    ax.bar(city, count, color=['orange', 'orange', 'orange', 'orange', 'orange'][i], alpha=alpha_values[i], linewidth=5)

# Add labels and title
ax.set_xlabel('City')
ax.set_ylabel('Number of Customer')
ax.set_title('Top 5 Cities with Highest Customer')

# Display the plot
plt.show()

unique_customer.customer_city.value_counts().nlargest(5)

unique_customer.customer_zip_code_prefix.value_counts().nlargest(5)

"""## **Top 5 Seller State, City, and Zip Code**"""

# Get the city counts for the top 5 cities
top_states = unique_seller['seller_state'].value_counts().nlargest(5)

# Define alpha values for each bar
alpha_values = [1.0, 0.7, 0.6, 0.5, 0.4]

# Plot the bar chart with the top 5 cities and adjusted alpha values
fig, ax = plt.subplots()

for i, (state, count) in enumerate(top_states.items()):
    ax.bar(state, count, color=['green', 'green', 'green', 'green', 'green'][i], alpha=alpha_values[i], linewidth=5)

# Add labels and title
ax.set_xlabel('State')
ax.set_ylabel('Number of Seller')
ax.set_title('Top 5 States with Highest Seller')

# Display the plot
plt.show()

unique_seller.seller_state.value_counts().nlargest(5)

# Get the city counts for the top 5 cities
top_cities = unique_seller['seller_city'].value_counts().nlargest(5)

# Define alpha values for each bar
alpha_values = [1.0, 0.7, 0.6, 0.5, 0.4]

# Plot the bar chart with the top 5 cities and adjusted alpha values
fig, ax = plt.subplots()

for i, (city, count) in enumerate(top_cities.items()):
    ax.bar(city, count, color=['green', 'green', 'green', 'green', 'green'][i], alpha=alpha_values[i], linewidth=5)

# Add labels and title
ax.set_xlabel('City')
ax.set_ylabel('Number of Seller')
ax.set_title('Top 5 Cities with Highest Seller')

# Display the plot
plt.show()

unique_seller.seller_city.value_counts().nlargest(5)

unique_seller.seller_zip_code_prefix.value_counts().nlargest(5)

"""## **Number of Customer Trx w/ Local Seller**"""

# Count occurrences based on City
city_count = (unique_customer['customer_city'] == unique_customer['seller_city']).sum()

# Count occurrences based on State
state_count = (unique_customer['customer_state'] == unique_customer['seller_state']).sum()

# Print the counts
print("Number of occurrences with the same City:", city_count)
print("Number of occurrences with the same State:", state_count)

"""## Which state generates the highest number of orders?"""

data_eda.groupby('customer_state')['order_id'].count().sort_values(ascending=False)

top_orders_states = data_eda.groupby("customer_state")["order_id"].count().reset_index().sort_values("order_id", ascending=False)
top_orders_states.rename(columns={"order_id":"count"}, inplace=True)
ax = sns.barplot(x="count", y="customer_state", data=top_orders_states[:5])
ax.set_title("TOP 5 states berdasarkan jumlah order")
plt.show()

"""## Which city has the highest number of orders?"""

data_eda.groupby('customer_city')['order_id'].count().sort_values(ascending=False)

top_orders_cities = data_eda.groupby("customer_city")["order_id"].count().reset_index().sort_values("order_id", ascending=False)
top_orders_cities.rename(columns={"order_id":"count"}, inplace=True)
ax = sns.barplot(x="count", y="customer_city", data=top_orders_cities[:5])
ax.set_title("TOP 5 cities berdasarkan jumlah order")

"""## Which state has the biggest revenue generation?"""

top_orval_states = data_eda.groupby("customer_state")["payment_value"].sum().reset_index().sort_values("payment_value", ascending=False)
top_orval_states["% of Total Payments"] = (top_orval_states["payment_value"] / top_orval_states["payment_value"].sum()) * 100
top_orval_states["Cum % of Total Payments"] = top_orval_states["% of Total Payments"].cumsum()

ax = sns.barplot(x="% of Total Payments", y="customer_state", data=top_orval_states[:5])
ax.set_title("TOP 5 states dengan revenue generation terbesar")
plt.show()

top_orval_states

"""## Which city has the biggest revenue generation?"""

top_orval_cities = data_eda.groupby("customer_city")["payment_value"].sum().reset_index().sort_values("payment_value", ascending=False)
top_orval_cities["% of Total Payments"] = (top_orval_cities["payment_value"] / top_orval_cities["payment_value"].sum()) * 100
top_orval_cities["Cum % of Total Payments"] = top_orval_cities["% of Total Payments"].cumsum()

ax = sns.barplot(x="% of Total Payments", y="customer_city", data=top_orval_cities[:5])
ax.set_title("TOP 5 cities dengan revenue generation terbesar")
plt.show()

top_orval_cities

"""## How is the distribution of time for customers to place orders?"""

data_eda["order_purchase_timestamp"] = pd.to_datetime(data_eda["order_purchase_timestamp"])

# Extract hour of the day
data_eda["hour_of_day"] = data_eda["order_purchase_timestamp"].dt.hour

# Define the order of hours
hour_order = range(24)

# Convert "hour_of_day" to categorical with custom order
data_eda["hour_of_day"] = pd.Categorical(data_eda["hour_of_day"], categories=hour_order, ordered=True)

# Group by hour of the day
orderbyhour = data_eda.groupby("hour_of_day")["order_id"].count().reset_index().sort_values(by="hour_of_day")

# Rename columns
orderbyhour.rename(columns={"order_id": "Total Orders", "hour_of_day": "Hour of Day"}, inplace=True)

plt.figure(figsize=(10, 5))
ax = sns.barplot(x="Hour of Day", y="Total Orders", data=orderbyhour)

# Add labels for each bar
for index, value in enumerate(orderbyhour["Total Orders"]):
    ax.text(index, value + 1, str(value), ha='center', va='bottom')

ax.set_title("Persebaran Order Berdasarkan Jam")
plt.show()

data_eda["order_purchase_timestamp"] = pd.to_datetime(data_eda["order_purchase_timestamp"])

# Extract day of the week
data_eda["day_of_week"] = data_eda["order_purchase_timestamp"].dt.day_name()

# Define the order of days
day_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]

# Convert "day_of_week" to categorical with custom order
data_eda["day_of_week"] = pd.Categorical(data_eda["day_of_week"], categories=day_order, ordered=True)

# Group by day of the week
orderbyday = data_eda.groupby("day_of_week")["order_id"].count().reset_index().sort_values(by="day_of_week")

# Rename columns
orderbyday.rename(columns={"order_id": "Total Orders", "day_of_week": "Day of Week"}, inplace=True)

plt.figure(figsize = (10,5))
ax = sns.barplot(x="Day of Week", y="Total Orders", data=orderbyday)
ax.set_xlabel('Day')
ax.set_title("Persebaran Jumlah Order Berdasarkan Hari")

# Add labels for each bar
for index, value in enumerate(orderbyday["Total Orders"]):
    ax.text(index, value + 1, str(value), ha='center', va='bottom')

plt.show()

data_eda["order_purchase_timestamp"] = pd.to_datetime(data_eda["order_purchase_timestamp"])

# Extract month of the year
data_eda["month_of_year"] = data_eda["order_purchase_timestamp"].dt.strftime('%B')

# Define the order of months
month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']

# Convert "month_of_year" to categorical with custom order
data_eda["month_of_year"] = pd.Categorical(data_eda["month_of_year"], categories=month_order, ordered=True)

# Group by month of the year
orderbymonth = data_eda.groupby("month_of_year")["order_id"].count().reset_index().sort_values(by="month_of_year")

# Rename columns
orderbymonth.rename(columns={"order_id": "Total Orders", "month_of_year": "Month of Year"}, inplace=True)

plt.figure(figsize=(10, 5))
ax = sns.barplot(x="Month of Year", y="Total Orders", data=orderbymonth)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha="right")  # Rotate month names
ax.set_xlabel('Month')
ax.set_title("Order Distribution by Month")

# Add labels for each bar
for index, value in enumerate(orderbymonth["Total Orders"]):
    ax.text(index, value + 1, str(value), ha='center', va='bottom')

plt.show()

"""## How is the distribution of product categories purchased by customers?"""

top_category = data_eda.groupby("product_category_name")["order_id"].count().reset_index().sort_values("order_id", ascending=False)
top_category.rename(columns={"order_id":"count"}, inplace=True)
ax = sns.barplot(x="count", y="product_category_name", data=top_category[:5])
ax.set_title("TOP 5 kategori produk berdasarkan jumlah order")
plt.show()

top_category.head()

"""## How is the distribution of products purchased by customers?"""

top_prod = data_eda.groupby("product_id")["order_id"].count().reset_index().sort_values("order_id", ascending=False)
top_prod.rename(columns={"order_id":"count"}, inplace=True)
ax = sns.barplot(x="count", y="product_id", data=top_prod[:5])
ax.set_title("TOP 5 produk berdasarkan jumlah order")
plt.show()

top_prod.head()

"""## **Purchased product size distribution**"""

data_eda.info()

data_eda.product_description_lenght.describe()

"""## How is the distribution of transaction prices by customers?"""

#Violin Plot with Rug Plot
plt.figure(figsize=(10, 6))
sns.violinplot(x=data_eda['price'], color='red', inner='quartile')
sns.rugplot(x=data_eda['price'], color='green', height=0.1)
plt.title('Violin Plot with Rug Plot of Transaction Prices')
plt.xlabel('Transaction Price (in BRL)')
plt.show()

data_eda['price'].describe()

"""## How is the distribution of freight value of customer transactions?"""

plt.figure(figsize=(10,5))
sns.histplot(x='freight_value', data=data_main)
plt.title('Distribusi Freight Value')
plt.xlabel('Freight Value')
plt.show()

"""It can be seen that the majority of freight value of transactions is spread below 100

## How is the distribution of customer payment options?
"""

plt.figure(figsize=(10, 5))
order = data_eda['payment_type'].value_counts().index  # Sort by count values
ax = sns.countplot(x='payment_type', data=data_eda, order=order)

# Add count labels for each bar
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Distribusi Tipe Pembayaran')
plt.xlabel('Tipe Pembayaran')
plt.ylabel('Jumlah Customer')
plt.show()

"""## How is the distribution of time for customers to make order reviews?"""

data_eda["review_creation_date"] = pd.to_datetime(data_eda["review_creation_date"])
reviewbyhour = data_eda.groupby(data_eda["review_creation_date"].dt.hour)["order_id"].count().reset_index().sort_values(by="review_creation_date", ascending=False)
reviewbyhour.rename(columns={"order_id":"Total Orders Reviewed", "review_creation_date": "Hour of Day"}, inplace=True)

plt.figure(figsize = (10,5))
ax = sns.barplot(x="Hour of Day", y="Total Orders Reviewed", data=reviewbyhour)
ax.set_title("Persebaran Order Review Berdasarkan Jam")
plt.show()

reviewbyday = data_eda.groupby(data_eda["review_creation_date"].dt.day_name())["order_id"].count().reset_index()
reviewbyday.rename(columns={"order_id":"Total Orders Reviewed", "review_creation_date": "Weekday Name"}, inplace=True)
reviewbyday = reviewbyday.sort_values(by="Total Orders Reviewed", ascending=False)

plt.figure(figsize = (10,5))
ax = sns.barplot(x="Weekday Name", y="Total Orders Reviewed", data=reviewbyday)
ax.set_xlabel('Day')
ax.set_title("Persebaran Order Review Berdasarkan Hari")
plt.show()

"""## How is the distribution of product assessments by customers?"""

#Top 5 review score
plt.figure(figsize=(12,12))
reviewscores = data_eda.groupby("product_category_name")["review_score"].agg(["mean", "count"]).sort_values(by="mean",ascending=False)
bestrated = reviewscores[reviewscores["count"]>=50][:5]
bestrated

#Bottom 5 review score
worstrated = reviewscores[reviewscores["count"]>=50].sort_values(by='mean')[:5]
worstrated

"""## How is the distribution of the review score by customers?"""

data_eda['review_score'].value_counts()

data_eda["review_score"].value_counts() / data_eda["review_score"].count() * 100

"""## How is the distribution of time for sellers to submit orders to the expedition?"""

data_eda["order_delivered_carrier_date"] = pd.to_datetime(data_eda["order_delivered_carrier_date"])

# Extract hour of the day
data_eda["hour_of_day"] = data_eda["order_delivered_carrier_date"].dt.hour

# Define the order of hours
hour_order = range(24)

# Convert "hour_of_day" to categorical with custom order
data_eda["hour_of_day"] = pd.Categorical(data_eda["hour_of_day"], categories=hour_order, ordered=True)

# Group by hour of the day
shipbyhour = data_eda.groupby("hour_of_day")["order_id"].count().reset_index().sort_values(by="hour_of_day")

# Rename columns
shipbyhour.rename(columns={"order_id": "Total Orders", "hour_of_day": "Hour of Day"}, inplace=True)

plt.figure(figsize=(10, 5))
ax = sns.barplot(x="Hour of Day", y="Total Orders", data=shipbyhour)

# Add labels for each bar
for index, value in enumerate(shipbyhour["Total Orders"]):
    ax.text(index, value + 1, str(value), ha='center', va='bottom')

ax.set_title("Persebaran Order Shipped Berdasarkan Jam")
plt.show()

data_eda["order_delivered_carrier_date"] = pd.to_datetime(data_eda["order_delivered_carrier_date"])

# Extract day of the week
data_eda["day_of_week"] = data_eda["order_delivered_carrier_date"].dt.day_name()

# Define the order of days
day_order = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]

# Convert "day_of_week" to categorical with custom order
data_eda["day_of_week"] = pd.Categorical(data_eda["day_of_week"], categories=day_order, ordered=True)

# Group by day of the week
shipbyday = data_eda.groupby("day_of_week")["order_id"].count().reset_index().sort_values(by="day_of_week")

# Rename columns
shipbyday.rename(columns={"order_id": "Total Orders", "day_of_week": "Day of Week"}, inplace=True)

plt.figure(figsize=(10, 5))
ax = sns.barplot(x="Day of Week", y="Total Orders", data=shipbyday)

# Add labels for each bar
for index, value in enumerate(shipbyday["Total Orders"]):
    ax.text(index, value + 1, str(value), ha='center', va='bottom')

ax.set_title("Persebaran Order Shipped Berdasarkan Hari Minggu")
plt.show()

data_eda["order_delivered_carrier_date"] = pd.to_datetime(data_eda["order_delivered_carrier_date"])

# Extract month
data_eda["month"] = data_eda["order_delivered_carrier_date"].dt.month_name()

# Define the order of months
month_order = [
    "January", "February", "March", "April", "May", "June",
    "July", "August", "September", "October", "November", "December"
]

# Convert "month" to categorical with custom order
data_eda["month"] = pd.Categorical(data_eda["month"], categories=month_order, ordered=True)

# Group by month
shipbymonth = data_eda.groupby("month")["order_id"].count().reset_index().sort_values(by="month")

# Rename columns
shipbymonth.rename(columns={"order_id": "Total Orders", "month": "Month"}, inplace=True)

plt.figure(figsize=(10, 5))
ax = sns.barplot(x="Month", y="Total Orders", data=shipbymonth)

# Add labels for each bar
for index, value in enumerate(shipbymonth["Total Orders"]):
    ax.text(index, value + 1, str(value), ha='center', va='bottom')

ax.set_title("Persebaran Order Shipped Berdasarkan Bulan")
plt.show()

"""## How long does it take on average for a seller to submit an order to an export after a customer places an order?"""

data_eda['diff_car_pur'] = (pd.to_datetime(data_eda.order_delivered_carrier_date) - pd.to_datetime(data_eda.order_purchase_timestamp)).dt.days

print(str.format('Rata-rata waktu yang dibutuhkan seller untuk menyerahkan barang ke ekspedisi setelah customer melakukan pemesanan adalah: {:.0f} hari', data_eda.diff_car_pur.mean()))

"""## How long does it take on average for a customer to review an order that has arrived after placing an order?"""

data_eda['diff_rev_pur'] = (pd.to_datetime(data_eda.review_creation_date) - pd.to_datetime(data_eda.order_purchase_timestamp)).dt.days

print(str.format('Rata-rata waktu yang dibutuhkan customer untuk review pesanan yang telah sampai setelah melakukan pemesanan adalah: {:.0f} hari', data_eda.diff_rev_pur.mean()))

"""## Is there a correlation between the duration of order delivery and the review score by the customer?"""

print(data_eda['diff_rev_pur'].corr(data_eda['review_score']))

"""# **RFM Analysis**

## **Recency, Frequency, Monetary**
"""

df = data_eda.copy()

time_columns= ['order_purchase_timestamp', 'order_approved_at','order_delivered_carrier_date','order_delivered_customer_date',
               'order_estimated_delivery_date', 'review_creation_date', 'review_answer_timestamp', 'shipping_limit_date']

df[time_columns]=df[time_columns].apply(pd.to_datetime)
df.info()

snapshot_date = df['order_purchase_timestamp'].max() + dt.timedelta(days=1)
print("Latest date in dataset + 1 day: ", snapshot_date)

recency= pd.DataFrame(df.groupby(by='customer_id', as_index=False)['order_purchase_timestamp'].max())
recency['recency']= recency['order_purchase_timestamp'].apply(lambda x: (snapshot_date - x).days)
recency.head()

# Group by customer_id and count the number of orders
frequency = df.groupby("customer_id").agg({"order_id": "count"}).reset_index()

# Rename columns
frequency.rename(columns={"order_id": "frequency"}, inplace=True)

frequency.head()

monetary = df.groupby('customer_id', as_index=False)['payment_value'].sum()
monetary.columns = ['customer_id', 'monetary']
monetary.head()

RFM = recency.merge(frequency, on='customer_id')
RFM = RFM.merge(monetary, on='customer_id').drop(columns='order_purchase_timestamp')
RFM.head()

RFM.info()

RFM.describe()

#Violin Plot with Rug Plot
plt.figure(figsize=(10, 6))
sns.violinplot(x=RFM['recency'], color='red', inner='quartile')
sns.rugplot(x=RFM['recency'], color='orange', height=0.1)
plt.title('Violin Plot with Rug Plot of Recency')
plt.xlabel('Recency')
plt.show()

#Violin Plot with Rug Plot
plt.figure(figsize=(10, 6))
sns.violinplot(x=RFM['frequency'], color='orange', inner='quartile')
sns.rugplot(x=RFM['frequency'], color='blue', height=0.1)
plt.title('Violin Plot with Rug Plot of Frequency')
plt.xlabel('Frequency')
plt.show()

#Violin Plot with Rug Plot
plt.figure(figsize=(10, 6))
sns.violinplot(x=RFM['monetary'], color='blue', inner='quartile')
sns.rugplot(x=RFM['monetary'], color='yellow', height=0.1)
plt.title('Violin Plot with Rug Plot of Monetary')
plt.xlabel('Monetary')
plt.show()

for i in ["recency", "frequency", "monetary"]:
    plt.figure()
    plt.tight_layout()
    plt.gca().set(xlabel= i, ylabel='Count')
    plt.boxplot(RFM[i])

"""## **Customer Segmentation**"""

rfm = RFM.copy()
rfm = rfm.set_index('customer_id')
rfm.head()

rfm["r"]  = pd.qcut(rfm['recency'], 5, labels=[5, 4, 3, 2, 1])
rfm["f"]= pd.qcut(rfm['frequency'].rank(method="first"), 5, labels=[1, 2, 3, 4, 5])
rfm["m"] = pd.qcut(rfm['monetary'], 5, labels=[1, 2, 3, 4, 5])

rfm['RFM_Concat'] = rfm.r.astype(str)+ rfm.f.astype(str) + rfm.m.astype(str)
rfm.head()

seg_map= {
    r'111|112|121|131|141|151': 'Lost',
    r'332|322|233|232|223|222|132|123|122|212|211': 'Hibernating',
    r'155|154|144|214|215|115|114|113': 'Cant Loose',
    r'255|254|245|244|253|252|243|242|235|234|225|224|153|152|145|143|142|135|134|133|125|124': 'At Risk',
    r'331|321|312|221|213|231|241|251': 'About To Sleep',
    r'535|534|443|434|343|334|325|324': 'Need Attention',
    r'525|524|523|522|521|515|514|513|425|424|413|414|415|315|314|313': 'Promising',
    r'512|511|422|421|412|411|311': 'New Customers',
    r'553|551|552|541|542|533|532|531|452|451|442|441|431|453|433|432|423|353|352|351|342|341|333|323': 'Potential Loyalist',
    r'543|444|435|355|354|345|344|335': 'Loyal',
    r'555|554|544|545|454|455|445': 'Champions'
}

rfm['segment'] = rfm['r'].astype(str) + rfm['f'].astype(str) + rfm['m'].astype(str)
rfm['segment'] = rfm['segment'].replace(seg_map, regex=True)
rfm.head()

"""For segmentation reference:

https://documentation.bloomreach.com/engagement/docs/rfm-segmentation
"""

rfmstats = rfm[["segment", "recency", "frequency", "monetary"]].groupby("segment").agg(['mean','median', 'min', 'max', 'count'])
rfmstats

rfmstats['ratio']= (100*rfmstats['monetary']["count"]/rfmstats['monetary']["count"].sum()).round(2)
rfmstats

plt.figure(figsize=(20,8))
#plt.rc('font', size=20)
per= sns.barplot(x=rfmstats['ratio'], y=rfmstats.index, data=rfmstats, palette="viridis")
sns.despine(bottom = True, left = True)
for i, v in enumerate(rfmstats['ratio']):
    per.text(v, i+.20,"  {:.2f}".format(v)+"%", color='black', ha="left")
per.set_ylabel('Segments', fontsize=25)
per.set(xticks=[])
plt.title('Distribution of Segments', fontsize=35)
plt.show()

!pip install squarify
import squarify

rfmstats.index

# Treemap by recency/frequency
plt.figure(figsize=(15,8))
plt.rc('font', size=15)
squarify.plot(sizes=rfmstats["recency"]["count"], label=rfmstats.index,
              color=["red","orange","blue", "gold", "cadetblue", "purple", "aqua","royalblue", "pink", "brown"], alpha=.55)
plt.suptitle("Recency Grid", fontsize=25);

rfm.segment.value_counts()

"""## **Churn Labeling**"""

df_rfm = rfm.copy()

df_rfm.reset_index(inplace=True)

df_rfm.drop(columns=['r','f','m','RFM_Concat'], inplace=True)

df_rfm[['recency','frequency','monetary']].describe()

import numpy as np

# Criteria for churn customers based on median
recency_threshold = 220
frequency_threshold = 1
monetary_threshold = 109

# Create a new column 'churn_label' based on the criteria
df_rfm['churn_label'] = np.where(
    (df_rfm['recency'] > recency_threshold) &
    (df_rfm['frequency'] == frequency_threshold) &
    (df_rfm['monetary'] < monetary_threshold),
    1,  # Churn label (1 for churned customers)
    0   # Non-churn label (0 for non-churned customers)
)

df_rfm.head()

df_rfm.drop(columns=['recency','frequency','monetary'], inplace=True)

df_rfm.churn_label.value_counts()

df_rfm.info()
df_rfm.head()

df_rfm[df_rfm['churn_label']==0].segment.value_counts()

df_rfm[df_rfm['churn_label']==1].segment.value_counts()

"""# Feature Engineering"""

df_churn = pd.merge(unique_customer,df_rfm,how='left',on='customer_id')

df_churn.info()
df_churn.head()

df_churn = df_churn[['customer_id','segment','churn_label']]

df_feature = data_eda.copy()

df_feature.info()

# Filter the dataframe for necessary columns
df_feature = df_feature[['customer_id','customer_state','customer_city','order_id','order_purchase_timestamp','order_approved_at',
                     'order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date',
                     'review_id','review_score','review_creation_date','review_answer_timestamp','order_item_id',
                     'product_id','seller_id','shipping_limit_date','freight_value','product_category_name',
                     'product_description_lenght','product_weight_g', 'price']]

# Convert timestamp columns to datetime format
df_feature['order_purchase_timestamp'] = pd.to_datetime(df_feature['order_purchase_timestamp'])
df_feature['order_approved_at'] = pd.to_datetime(df_feature['order_approved_at'])
df_feature['order_delivered_carrier_date'] = pd.to_datetime(df_feature['order_delivered_carrier_date'])
df_feature['order_delivered_customer_date'] = pd.to_datetime(df_feature['order_delivered_customer_date'])
df_feature['order_estimated_delivery_date'] = pd.to_datetime(df_feature['order_estimated_delivery_date'])
df_feature['review_creation_date'] = pd.to_datetime(df_feature['review_creation_date'])
df_feature['review_answer_timestamp'] = pd.to_datetime(df_feature['review_answer_timestamp'])
df_feature['shipping_limit_date'] = pd.to_datetime(df_feature['shipping_limit_date'])

"""## **Average time between estimated and actual delivery, review score, and freight value**"""

df_feature['diff_estimated_actual_delivery'] = (df_feature['order_delivered_customer_date'] - df_feature['order_estimated_delivery_date']).dt.days

df_feature.diff_estimated_actual_delivery.describe()

import pandas as pd

# List of columns to aggregate
columns_to_aggregate = ['diff_estimated_actual_delivery',
                        'review_score','freight_value','price']

# Initialize an empty DataFrame to store the aggregated results
aggregated_results = pd.DataFrame()

# Perform aggregation for each specified column
for column in columns_to_aggregate:
    # Aggregate the calculated time for each unique customer
    customer_agg = df_feature.groupby('customer_id')[column].mean().reset_index()

    # Create new attributes for the unique customer DataFrame based on the aggregated values
    df_unique_customers = df_feature.drop_duplicates(subset='customer_id')[['customer_id']]
    df_unique_customers = pd.merge(df_unique_customers, customer_agg, on='customer_id', how='left')

    # Rename the column appropriately
    df_unique_customers.rename(columns={column: f'avg_{column}'}, inplace=True)

    # Merge the results into the aggregated_results DataFrame
    if aggregated_results.empty:
        aggregated_results = df_unique_customers
    else:
        aggregated_results = pd.merge(aggregated_results, df_unique_customers, on='customer_id', how='left')

aggregated_results.info()
aggregated_results.head()

"""## **Customer level**"""

data_feature = pd.merge(aggregated_results, df_churn, on='customer_id', how='left')

data_feature.info()

# Applying the conditions to create 'prone_churn_segment' column
data_feature['prone_churn_segment'] = data_feature['segment'].apply(lambda x: 1 if x in ['Lost', 'Hibernating', 'About To Sleep'] else 0)

data_feature.drop(columns='segment',inplace=True)

data_feature.info()
data_feature.head()

data_feature[['customer_id','prone_churn_segment']].tail(5)

"""# **Feature Correlation**"""

data_feature.corr()

"""# **EDA After Churn Labeling**"""

unique_customer.info()

data_feature.info()

ec = pd.merge(unique_customer, df_rfm, on='customer_id', how='left')

ec.drop(columns='churn_label', inplace=True)

eda_churn = pd.merge(ec, data_feature, on='customer_id', how='left')

eda_churn.info()

no_churn = eda_churn[eda_churn['churn_label']==0]
yes_churn = eda_churn[eda_churn['churn_label']==1]

print(len(no_churn))
print(len(yes_churn))

"""## **Average Diff Estimated-Actual Delivery, Review Score, Freight Value, and Price**"""

yes_churn[['avg_diff_estimated_actual_delivery','avg_review_score',
           'avg_freight_value','avg_price']].describe()

"""## **Demographic**"""

# Calculate the churn proportion for each customer state
state_churn_proportion = (yes_churn.groupby('customer_state')['churn_label'].count() /
                          (yes_churn.groupby('customer_state')['churn_label'].count() +
                           no_churn.groupby('customer_state')['churn_label'].count()))

# Display the top 5 states based on churn proportion
print(state_churn_proportion.nlargest(5))

# Calculate the churn proportion for each customer city
city_churn_proportion = (yes_churn.groupby('customer_city')['churn_label'].count() /
                          (yes_churn.groupby('customer_city')['churn_label'].count() +
                           no_churn.groupby('customer_city')['churn_label'].count()))

# Display the top 5 cities based on churn proportion
print(city_churn_proportion.nlargest(5))

eda_churn[eda_churn['customer_state']=='RR'].customer_id.count()

eda_churn[eda_churn['customer_city']=='juruaia'].customer_id.count()

print(yes_churn.customer_state.value_counts().nlargest(5))
print(yes_churn.customer_city.value_counts().nlargest(5))

print(yes_churn.customer_state.value_counts().nlargest(5)/
 (yes_churn.customer_state.value_counts().nlargest(5)+no_churn.customer_state.value_counts().nlargest(5)))
print(yes_churn.customer_city.value_counts().nlargest(5)/
      (yes_churn.customer_city.value_counts().nlargest(5)+no_churn.customer_city.value_counts().nlargest(5)))

"""## **Customer Segment**"""

print(yes_churn.segment.value_counts())

"""## **Payment Type**"""

no_churn.payment_type.value_counts()

yes_churn.payment_type.value_counts()

"""# **Preparation**"""

data_model = data_feature.copy()

data_model.describe()

# Define function to handle outliers using IQR
def adjust_outliers(df, columns):
    result = df.copy()
    for feature_name in columns:
        Q1 = df[feature_name].quantile(0.25)
        Q3 = df[feature_name].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Adjust values below the lower bound
        result[feature_name] = result[feature_name].apply(lambda x: lower_bound if x < lower_bound else x)

        # Adjust values above the upper bound
        result[feature_name] = result[feature_name].apply(lambda x: upper_bound if x > upper_bound else x)

    return result

# Apply outlier adjustment to selected columns
data_model = adjust_outliers(data_model, ['avg_diff_estimated_actual_delivery', 'avg_price',
                     'avg_review_score', 'avg_freight_value'])

# List of columns to normalize
cols_to_normalize = ['avg_diff_estimated_actual_delivery', 'avg_price',
                     'avg_review_score', 'avg_freight_value']

# Apply min-max normalization to each column
for col in cols_to_normalize:
    data_model[col] = (data_model[col] - data_model[col].min()) / (data_model[col].max() - data_model[col].min())

data_model.tail()

"""# Modelling"""

X1 = data_model.drop(['customer_id','churn_label'], axis=1)
Y1 = data_model['churn_label']

from sklearn import svm
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
from xgboost import XGBClassifier
import time

"""## **Split and SMOTE**"""

# split the dataset into training and testing sets
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, Y1, test_size=0.2, random_state=42)

# Apply SMOTE only to the training set
smote = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X1_train, y1_train)

"""## **Random Forest**"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import numpy as np

# Create a Random Forest classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Define the parameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': randint(50, 200),  # Randomly choose between 50 and 200 trees
    'max_depth': [None] + list(randint(3, 20).rvs(5)),  # Randomly choose between 3 and 20 for tree depth
    'min_samples_split': randint(2, 11),  # Randomly choose between 2 and 10 for min samples split
    'min_samples_leaf': randint(1, 11),  # Randomly choose between 1 and 10 for min samples leaf
    'max_features': ['sqrt', 'log2', None]  # Adjusted based on the warning
}

# Create RandomizedSearchCV
random_search = RandomizedSearchCV(
    rf_classifier, param_distributions=param_dist, n_iter=5, cv=3, scoring='f1_weighted', random_state=42
)

# Fit the RandomizedSearchCV to the training data
random_search.fit(X_train_smote, y_train_smote)

# Print the best hyperparameters
print("Best Hyperparameters:", random_search.best_params_)

# Use the best estimator for predictions
best_rf_classifier = random_search.best_estimator_
y_pred = best_rf_classifier.predict(X1_test)
y_pred_train = best_rf_classifier.predict(X_train_smote)

# Evaluate the performance on test set
accuracy = accuracy_score(y1_test, y_pred)
precision = precision_score(y1_test, y_pred, average='weighted')
recall = recall_score(y1_test, y_pred, average='weighted')
f1 = f1_score(y1_test, y_pred, average='weighted')
conf_matrix = confusion_matrix(y1_test, y_pred)

# Evaluate the performance on train set
acc_train = accuracy_score(y_train_smote, y_pred_train)
prec_train = precision_score(y_train_smote, y_pred_train, average='weighted')
rec_train = recall_score(y_train_smote, y_pred_train, average='weighted')
f1_train = f1_score(y_train_smote, y_pred_train, average='weighted')
cm_train = confusion_matrix(y_train_smote, y_pred_train)

# Print the results
print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-score: {f1:.4f}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(classification_report(y1_test, y_pred))

# Print the results on train
print(f'Accuracy Train: {acc_train:.4f}')
print(f'Precision Train: {prec_train:.4f}')
print(f'Recall Train: {rec_train:.4f}')
print(f'F1-score Train: {f1_train:.4f}')
print(f'Confusion Matrix Train:\n{cm_train}')
print(classification_report(y_train_smote, y_pred_train))

# Get feature importances
feature_importances = best_rf_classifier.feature_importances_

# Create a DataFrame to display feature importances
feature_importance_df = pd.DataFrame({'Feature': X_train_smote.columns, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

feature_importance_df

"""## **XGBoost**"""

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# Create an XGBoost classifier
xgb_classifier = XGBClassifier(random_state=42)

# Define the parameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': np.arange(50, 200),  # Adjust the range based on your preference
    'max_depth': np.arange(3, 20),  # Adjust the range based on your preference
    'learning_rate': np.logspace(-3, 0, 100),  # Adjust the range based on your preference
    'subsample': np.linspace(0.5, 1.0, 10),  # Adjust the range based on your preference
    'colsample_bytree': np.linspace(0.5, 1.0, 10),  # Adjust the range based on your preference
    'gamma': np.arange(0, 5),  # Adjust the range based on your preference
}

# Create RandomizedSearchCV with early stopping
random_search_xgb = RandomizedSearchCV(
    xgb_classifier, param_distributions=param_dist, n_iter=5, cv=3, scoring='f1_weighted', random_state=42,
    n_jobs=-1,  # Use all available CPUs for parallel processing
    verbose=2  # Increase verbosity to see progress
)

# Fit the RandomizedSearchCV to the training data
random_search_xgb.fit(X_train_smote, y_train_smote, eval_metric="logloss", early_stopping_rounds=10, eval_set=[(X1_test, y1_test)])

# Print the best hyperparameters
print("Best Hyperparameters:", random_search_xgb.best_params_)

# Use the best estimator for predictions
best_xgb_classifier = random_search_xgb.best_estimator_
y_pred = best_xgb_classifier.predict(X1_test)
y_pred_train = best_xgb_classifier.predict(X_train_smote)

# Evaluate the performance on test set
accuracy = accuracy_score(y1_test, y_pred)
precision = precision_score(y1_test, y_pred, average='weighted')
recall = recall_score(y1_test, y_pred, average='weighted')
f1 = f1_score(y1_test, y_pred, average='weighted')
conf_matrix = confusion_matrix(y1_test, y_pred)

# Evaluate the performance on train set
acc_train = accuracy_score(y_train_smote, y_pred_train)
prec_train = precision_score(y_train_smote, y_pred_train, average='weighted')
rec_train = recall_score(y_train_smote, y_pred_train, average='weighted')
f1_train = f1_score(y_train_smote, y_pred_train, average='weighted')
cm_train = confusion_matrix(y_train_smote, y_pred_train)

# Print the results
print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1-score: {f1:.4f}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(classification_report(y1_test, y_pred))

# Print the results on train
print(f'Accuracy Train: {acc_train:.4f}')
print(f'Precision Train: {prec_train:.4f}')
print(f'Recall Train: {rec_train:.4f}')
print(f'F1-score Train: {f1_train:.4f}')
print(f'Confusion Matrix Train:\n{cm_train}')
print(classification_report(y_train_smote, y_pred_train))

# Get feature importances
feature_importances2 = best_xgb_classifier.feature_importances_

# Create a DataFrame to display feature importances
feature_importance_df2 = pd.DataFrame({'Feature': X_train_smote.columns, 'Importance': feature_importances2})
feature_importance_df2 = feature_importance_df2.sort_values(by='Importance', ascending=False)

feature_importance_df2